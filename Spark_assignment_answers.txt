1. Read a CSV and print output to console 
ANS:
df_cars = spark.read.csv('/FileStore/Sample_data/CARS.csv', header=True, inferSchema=True)
df_cars.show()


2. Read a JSON file and print output to console 
ANS:
df_households = spark.read.json('/FileStore/Sample_data/Households.json')
df_households.show()


3. Read parquet file and print output to console 
ANS:
df_parquet = spark.read.parquet('/FileStore/Sample_data/cleaned_data.parquet')
df_parquet.show()


4. Read a avro file and print output to console 
ANS:
df_avro = spark.read.format('avro').load('/FileStore/Sample_data/cleaned_data1.avro')
df_avro.show()


5. Example for broadcast join (Inner join 2 dataframes)
ANS:
from pyspark.sql import functions as F

df_customer = spark.read.csv('/FileStore/Sample_data/CUSTOMER.csv', header=True, inferSchema=True)
df_broadcast_join = df_cars.join(F.broadcast(df_customer), df_cars.id == df_customer.id, 'inner')
df_broadcast_join.show()


6. Example for Filtering the data 
ANS:
df_filtered = df_customer.filter(df_customer['age'] > 30)
df_filtered.show()


7. Example for applying aggregate functions like  max, min, avg 
ANS:
df_customer.agg(F.max("salary").alias("Max Salary"), F.min("salary").alias("Min Salary"), F.avg("salary").alias("Average Salary")).show()


8. Example for Read json file with typed schema (without infering the schema) with Structtype, StructField .....
ANS:
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("name", StringType(), True),
    StructField("dob", StringType(), True),
    StructField("phone", IntegerType(), True)
])

df_typed_json = spark.read.schema(schema).json('/FileStore/Sample_data/Households.json')
df_typed_json.show()


9. Example for increase and decrease number of dataframe partitions 
ANS:
df_repartitioned = df_cars.repartition(10)  # Increase partitions to 10
df_coalesced = df_cars.coalesce(2)  # Decrease partitions to 2


10. Example for renaming the column of the dataframe 
ANS:
df_renamed = df_cars.withColumnRenamed("old_column_name", "new_column_name")
df_renamed.show()


11. Example for adding a new column to the dataframe 
ANS:
df_with_new_col = df_cars.withColumn("new_column", F.lit("new_value"))
df_with_new_col.show()


12. Changing the structure of the dataframe 

sample json: 
[
"""{ "name" : "john doe", "dob" : "01-01-1980" }""",
"""{ "name" : "john adam", "dob" : "01-01-1960", "phone" : 1234567890 }"""
]

A. Comeup with your own data , add more data points 
ANS:
[
  { "name" : "john doe", "dob" : "01-01-1980" },
  { "name" : "john adam", "dob" : "01-01-1960", "phone" : 1234567890 },
  { "name" : "jane doe", "dob" : "05-15-1990" },
  { "name" : "michael smith", "dob" : "07-07-1975", "phone" : 9876543210 }
]


B. I want you to read the above dataset into dataframe 
ANS:
# I have created a JSON file with the data
df_json = spark.read.json('/FileStore/Sample_data/Households.json')
df_json.show()

C. Now, change the dataframe structure and write the dataframe as json file 
Json file structure should look like 

{
{ "personal_data" : { "name" : "john doe", "dob" : "01-01-1980" } } , 
{ "personal_data" : { "name" : "john adam", "dob" : "01-01-1960", "phone" : 1234567890 } }
}

hint: read string as rdd , use struct function on dataframe 
ANS:
from pyspark.sql import functions as F

# Create a new column 'personal_data' with a struct that contains 'name', 'dob', and 'phone'
df_structured = df_json.withColumn("personal_data", F.struct("name", "dob", "phone")).select("personal_data")

# Show the transformed structure
df_structured.show(truncate=False)

# Write the structured DataFrame back to a JSON file
df_structured.write.json('/FileStore/Sample_data/output_data.json')



13. Read from kafka source and print output as Stream 

This requires local setup of Kafka. Make sure you do some reasearch about kafka in youtube. 

I am able to setup kafka in my windows laptop. Make sure you install gitbash. Check intructions for kafka setup file 


